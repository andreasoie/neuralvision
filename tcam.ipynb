{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.io.image import read_image\n",
    "from torchvision.transforms.functional import normalize, resize, to_pil_image\n",
    "from torchvision.models import resnet18\n",
    "from torchcam.methods import SmoothGradCAMpp\n",
    "from scripts.save_comparison_images import get_config, get_trained_model, get_dataloader, create_prediction_image\n",
    "from core.tops.config.instantiate import instantiate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading config from: core/configs/task4/retina_P4_retrain_overfit.py\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-28 10:32:03,798 [INFO ] Loaded checkpoint from outputs/configs/task4/retina_P4_retrain_overfit/checkpoints/4019.ckpt\n"
     ]
    }
   ],
   "source": [
    "CFG_PATH = \"core/configs/task4/retina_P4_retrain_overfit.py\"\n",
    "\n",
    "cfg = get_config(CFG_PATH)\n",
    "model = get_trained_model(cfg)\n",
    "\n",
    "train = True\n",
    "\n",
    "if train:\n",
    "    dataset_to_visualize = \"train\"\n",
    "else:\n",
    "    dataset_to_visualize = \"val\"\n",
    "\n",
    "dataloader = iter(get_dataloader(cfg, dataset_to_visualize))\n",
    "\n",
    "img_transform = instantiate(cfg.data_val.gpu_transform)\n",
    "\n",
    "score_threshold = 0.99\n",
    "\n",
    "batch = next(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = batch[\"image\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 128, 1024])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/andy/dev/neuralvision/tcam.ipynb Cell 5'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/andy/dev/neuralvision/tcam.ipynb#ch0000005?line=0'>1</a>\u001b[0m cam_extractor \u001b[39m=\u001b[39m SmoothGradCAMpp(model\u001b[39m.\u001b[39;49mfeature_extractor)\n",
      "File \u001b[0;32m~/dev/neuralvision/venv/lib/python3.8/site-packages/torchcam/methods/gradient.py:241\u001b[0m, in \u001b[0;36mSmoothGradCAMpp.__init__\u001b[0;34m(self, model, target_layer, num_samples, std, input_shape, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/andy/dev/neuralvision/venv/lib/python3.8/site-packages/torchcam/methods/gradient.py?line=230'>231</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m    <a href='file:///home/andy/dev/neuralvision/venv/lib/python3.8/site-packages/torchcam/methods/gradient.py?line=231'>232</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    <a href='file:///home/andy/dev/neuralvision/venv/lib/python3.8/site-packages/torchcam/methods/gradient.py?line=232'>233</a>\u001b[0m     model: nn\u001b[39m.\u001b[39mModule,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/andy/dev/neuralvision/venv/lib/python3.8/site-packages/torchcam/methods/gradient.py?line=237'>238</a>\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    <a href='file:///home/andy/dev/neuralvision/venv/lib/python3.8/site-packages/torchcam/methods/gradient.py?line=238'>239</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/andy/dev/neuralvision/venv/lib/python3.8/site-packages/torchcam/methods/gradient.py?line=240'>241</a>\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(model, target_layer, input_shape, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/andy/dev/neuralvision/venv/lib/python3.8/site-packages/torchcam/methods/gradient.py?line=241'>242</a>\u001b[0m     \u001b[39m# Model scores is not used by the extractor\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/andy/dev/neuralvision/venv/lib/python3.8/site-packages/torchcam/methods/gradient.py?line=242'>243</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_score_used \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/neuralvision/venv/lib/python3.8/site-packages/torchcam/methods/gradient.py:35\u001b[0m, in \u001b[0;36m_GradCAM.__init__\u001b[0;34m(self, model, target_layer, input_shape, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///home/andy/dev/neuralvision/venv/lib/python3.8/site-packages/torchcam/methods/gradient.py?line=26'>27</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m     <a href='file:///home/andy/dev/neuralvision/venv/lib/python3.8/site-packages/torchcam/methods/gradient.py?line=27'>28</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     <a href='file:///home/andy/dev/neuralvision/venv/lib/python3.8/site-packages/torchcam/methods/gradient.py?line=28'>29</a>\u001b[0m     model: nn\u001b[39m.\u001b[39mModule,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='file:///home/andy/dev/neuralvision/venv/lib/python3.8/site-packages/torchcam/methods/gradient.py?line=31'>32</a>\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m     <a href='file:///home/andy/dev/neuralvision/venv/lib/python3.8/site-packages/torchcam/methods/gradient.py?line=32'>33</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> <a href='file:///home/andy/dev/neuralvision/venv/lib/python3.8/site-packages/torchcam/methods/gradient.py?line=34'>35</a>\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(model, target_layer, input_shape, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     <a href='file:///home/andy/dev/neuralvision/venv/lib/python3.8/site-packages/torchcam/methods/gradient.py?line=35'>36</a>\u001b[0m     \u001b[39m# Init hook\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/andy/dev/neuralvision/venv/lib/python3.8/site-packages/torchcam/methods/gradient.py?line=36'>37</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhook_g: List[Tensor] \u001b[39m=\u001b[39m [\u001b[39mNone\u001b[39;00m] \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_names)  \u001b[39m# type: ignore[list-item]\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/neuralvision/venv/lib/python3.8/site-packages/torchcam/methods/core.py:54\u001b[0m, in \u001b[0;36m_CAM.__init__\u001b[0;34m(self, model, target_layer, input_shape, enable_hooks)\u001b[0m\n\u001b[1;32m     <a href='file:///home/andy/dev/neuralvision/venv/lib/python3.8/site-packages/torchcam/methods/core.py?line=47'>48</a>\u001b[0m     target_names \u001b[39m=\u001b[39m [\n\u001b[1;32m     <a href='file:///home/andy/dev/neuralvision/venv/lib/python3.8/site-packages/torchcam/methods/core.py?line=48'>49</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_resolve_layer_name(layer) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(layer, nn\u001b[39m.\u001b[39mModule) \u001b[39melse\u001b[39;00m layer\n\u001b[1;32m     <a href='file:///home/andy/dev/neuralvision/venv/lib/python3.8/site-packages/torchcam/methods/core.py?line=49'>50</a>\u001b[0m         \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m target_layer\n\u001b[1;32m     <a href='file:///home/andy/dev/neuralvision/venv/lib/python3.8/site-packages/torchcam/methods/core.py?line=50'>51</a>\u001b[0m     ]\n\u001b[1;32m     <a href='file:///home/andy/dev/neuralvision/venv/lib/python3.8/site-packages/torchcam/methods/core.py?line=51'>52</a>\u001b[0m \u001b[39melif\u001b[39;00m target_layer \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     <a href='file:///home/andy/dev/neuralvision/venv/lib/python3.8/site-packages/torchcam/methods/core.py?line=52'>53</a>\u001b[0m     \u001b[39m# If the layer is not specified, try automatic resolution\u001b[39;00m\n\u001b[0;32m---> <a href='file:///home/andy/dev/neuralvision/venv/lib/python3.8/site-packages/torchcam/methods/core.py?line=53'>54</a>\u001b[0m     target_name \u001b[39m=\u001b[39m locate_candidate_layer(model, input_shape)  \u001b[39m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/andy/dev/neuralvision/venv/lib/python3.8/site-packages/torchcam/methods/core.py?line=54'>55</a>\u001b[0m     \u001b[39m# Warn the user of the choice\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/andy/dev/neuralvision/venv/lib/python3.8/site-packages/torchcam/methods/core.py?line=55'>56</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(target_name, \u001b[39mstr\u001b[39m):\n",
      "File \u001b[0;32m~/dev/neuralvision/venv/lib/python3.8/site-packages/torchcam/methods/_utils.py:43\u001b[0m, in \u001b[0;36mlocate_candidate_layer\u001b[0;34m(mod, input_shape)\u001b[0m\n\u001b[1;32m     <a href='file:///home/andy/dev/neuralvision/venv/lib/python3.8/site-packages/torchcam/methods/_utils.py?line=40'>41</a>\u001b[0m \u001b[39m# forward empty\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/andy/dev/neuralvision/venv/lib/python3.8/site-packages/torchcam/methods/_utils.py?line=41'>42</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> <a href='file:///home/andy/dev/neuralvision/venv/lib/python3.8/site-packages/torchcam/methods/_utils.py?line=42'>43</a>\u001b[0m     _ \u001b[39m=\u001b[39m mod(torch\u001b[39m.\u001b[39;49mrand(\u001b[39m1\u001b[39;49m, \u001b[39m*\u001b[39;49minput_shape, device\u001b[39m=\u001b[39;49m\u001b[39mnext\u001b[39;49m(mod\u001b[39m.\u001b[39;49mparameters())\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mdevice))\n\u001b[1;32m     <a href='file:///home/andy/dev/neuralvision/venv/lib/python3.8/site-packages/torchcam/methods/_utils.py?line=44'>45</a>\u001b[0m \u001b[39m# Remove all temporary hooks\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/andy/dev/neuralvision/venv/lib/python3.8/site-packages/torchcam/methods/_utils.py?line=45'>46</a>\u001b[0m \u001b[39mfor\u001b[39;00m handle \u001b[39min\u001b[39;00m hook_handles:\n",
      "File \u001b[0;32m~/dev/neuralvision/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1120\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/andy/dev/neuralvision/venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1116'>1117</a>\u001b[0m     bw_hook \u001b[39m=\u001b[39m hooks\u001b[39m.\u001b[39mBackwardHook(\u001b[39mself\u001b[39m, full_backward_hooks)\n\u001b[1;32m   <a href='file:///home/andy/dev/neuralvision/venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1117'>1118</a>\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m bw_hook\u001b[39m.\u001b[39msetup_input_hook(\u001b[39minput\u001b[39m)\n\u001b[0;32m-> <a href='file:///home/andy/dev/neuralvision/venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1119'>1120</a>\u001b[0m result \u001b[39m=\u001b[39m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/andy/dev/neuralvision/venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1120'>1121</a>\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n\u001b[1;32m   <a href='file:///home/andy/dev/neuralvision/venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1121'>1122</a>\u001b[0m     \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m (\u001b[39m*\u001b[39m_global_forward_hooks\u001b[39m.\u001b[39mvalues(), \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n",
      "File \u001b[0;32m~/dev/neuralvision/core/backbones/resnetfpn.py:41\u001b[0m, in \u001b[0;36mResnetFPN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='file:///home/andy/dev/neuralvision/core/backbones/resnetfpn.py?line=39'>40</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m TensorTuple:\n\u001b[0;32m---> <a href='file:///home/andy/dev/neuralvision/core/backbones/resnetfpn.py?line=40'>41</a>\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbackbone(x)\n\u001b[1;32m     <a href='file:///home/andy/dev/neuralvision/core/backbones/resnetfpn.py?line=41'>42</a>\u001b[0m     out \u001b[39m=\u001b[39m tensors_to_dict(out)\n\u001b[1;32m     <a href='file:///home/andy/dev/neuralvision/core/backbones/resnetfpn.py?line=42'>43</a>\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfpn(out)\n",
      "File \u001b[0;32m~/dev/neuralvision/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1123\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/andy/dev/neuralvision/venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1120'>1121</a>\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n\u001b[1;32m   <a href='file:///home/andy/dev/neuralvision/venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1121'>1122</a>\u001b[0m     \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m (\u001b[39m*\u001b[39m_global_forward_hooks\u001b[39m.\u001b[39mvalues(), \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[0;32m-> <a href='file:///home/andy/dev/neuralvision/venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1122'>1123</a>\u001b[0m         hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39;49m, \u001b[39minput\u001b[39;49m, result)\n\u001b[1;32m   <a href='file:///home/andy/dev/neuralvision/venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1123'>1124</a>\u001b[0m         \u001b[39mif\u001b[39;00m hook_result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   <a href='file:///home/andy/dev/neuralvision/venv/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1124'>1125</a>\u001b[0m             result \u001b[39m=\u001b[39m hook_result\n",
      "File \u001b[0;32m~/dev/neuralvision/venv/lib/python3.8/site-packages/torchcam/methods/_utils.py:34\u001b[0m, in \u001b[0;36mlocate_candidate_layer.<locals>._record_output_shape\u001b[0;34m(module, input, output, name)\u001b[0m\n\u001b[1;32m     <a href='file:///home/andy/dev/neuralvision/venv/lib/python3.8/site-packages/torchcam/methods/_utils.py?line=31'>32</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_record_output_shape\u001b[39m(module: nn\u001b[39m.\u001b[39mModule, \u001b[39minput\u001b[39m: Tensor, output: Tensor, name: Optional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     <a href='file:///home/andy/dev/neuralvision/venv/lib/python3.8/site-packages/torchcam/methods/_utils.py?line=32'>33</a>\u001b[0m     \u001b[39m\"\"\"Activation hook\"\"\"\u001b[39;00m\n\u001b[0;32m---> <a href='file:///home/andy/dev/neuralvision/venv/lib/python3.8/site-packages/torchcam/methods/_utils.py?line=33'>34</a>\u001b[0m     output_shapes\u001b[39m.\u001b[39mappend((name, output\u001b[39m.\u001b[39;49mshape))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "cam_extractor = SmoothGradCAMpp(model.feature_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get your input\n",
    "# img = read_image(\"path/to/your/image.png\")\n",
    "# Preprocess it for your chosen model\n",
    "# input_tensor = normalize(resize(img, (224, 224)) / 255., [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "\n",
    "# Preprocess your data and feed it to the model\n",
    "out = model.feature_extractor(img)\n",
    "# Retrieve the CAM by passing the class index and the model output\n",
    "activation_map = cam_extractor(out.squeeze(0).argmax().item(), out)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "64a4548cfd3509a00ab45dd82e9ec2cddcf0622da5c641a16d2fecd7ebf905b3"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
